{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "CaKEQ0a4IRkK",
        "GnphuygzjsPg",
        "cp-dPOAPIo2a",
        "DK-gq00-lFle",
        "9KGxIpG7l8Dy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocess data"
      ],
      "metadata": {
        "id": "7u4_B0tufpdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 187x5YSXYibG4QwC5m_Hx8cNzPGVTXv6G\n",
        "!unzip -q data.zip\n",
        "!rm data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmrYSK9oLVvX",
        "outputId": "6f25ca73-4bdf-42a8-a5ec-c56a9332ce9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=187x5YSXYibG4QwC5m_Hx8cNzPGVTXv6G\n",
            "From (redirected): https://drive.google.com/uc?id=187x5YSXYibG4QwC5m_Hx8cNzPGVTXv6G&confirm=t&uuid=803b2455-9b6e-4159-b434-21d946773866\n",
            "To: /content/data.zip\n",
            "100% 2.73G/2.73G [00:28<00:00, 96.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "cA4P16Jjd0VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHKTIlSge8HS"
      },
      "outputs": [],
      "source": [
        "class ColorDataset(Dataset):\n",
        "    def __init__(self, out_directory, listdir=None, featslistdir=None, shape=(64, 64), outshape=(256, 256), split=\"train\"):\n",
        "\n",
        "        # Save paths to a list\n",
        "        self.img_fns = []\n",
        "        self.feats_fns = []\n",
        "\n",
        "        with open(\"%s/list.%s.vae.txt\" % (listdir, split), \"r\") as ftr:\n",
        "            for img_fn in ftr:\n",
        "                self.img_fns.append(img_fn.strip(\"\\n\"))\n",
        "\n",
        "        with open(\"%s/list.%s.txt\" % (featslistdir, split), \"r\") as ftr:\n",
        "            for feats_fn in ftr:\n",
        "                self.feats_fns.append(feats_fn.strip(\"\\n\"))\n",
        "\n",
        "        self.img_num = min(len(self.img_fns), len(self.feats_fns))\n",
        "        self.shape = shape\n",
        "        self.outshape = outshape\n",
        "        self.out_directory = out_directory\n",
        "\n",
        "        # Create a dictionary to save weight of 313 ab bins\n",
        "        self.lossweights = None\n",
        "        countbins = 1.0 / np.load(\"data/zhang_weights/prior_probs.npy\")\n",
        "        binedges = np.load(\"data/zhang_weights/ab_quantize.npy\").reshape(2, 313)\n",
        "        lossweights = {}\n",
        "        for i in range(313):\n",
        "            if binedges[0, i] not in lossweights:\n",
        "                lossweights[binedges[0, i]] = {}\n",
        "            lossweights[binedges[0, i]][binedges[1, i]] = countbins[i]\n",
        "        self.binedges = binedges\n",
        "        self.lossweights = lossweights\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.img_num\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Declare empty arrays to get values\n",
        "        color_ab = np.zeros((2, self.shape[0], self.shape[1]), dtype=\"f\")\n",
        "        weights = np.ones((2, self.shape[0], self.shape[1]), dtype=\"f\")\n",
        "        recon_const = np.zeros((1, self.shape[0], self.shape[1]), dtype=\"f\")\n",
        "        recon_const_outres = np.zeros((1, self.outshape[0], self.outshape[1]), dtype=\"f\")\n",
        "        greyfeats = np.zeros((512, 28, 28), dtype=\"f\")\n",
        "\n",
        "        # Read and reshape\n",
        "        img_large = cv2.imread(self.img_fns[idx])\n",
        "        if self.shape is not None:\n",
        "            img = cv2.resize(img_large, (self.shape[0], self.shape[1]))\n",
        "            img_outres = cv2.resize(img_large, (self.outshape[0], self.outshape[1]))\n",
        "\n",
        "        # Convert BGR to LAB\n",
        "        img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        img_lab_outres = cv2.cvtColor(img_outres, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "        # Normalize to [-1..1]\n",
        "        img_lab = ((img_lab * 2.0) / 255.0) - 1.0\n",
        "        img_lab_outres = ((img_lab_outres * 2.0) / 255.0) - 1.0\n",
        "\n",
        "        recon_const[0, :, :] = img_lab[..., 0]\n",
        "        recon_const_outres[0, :, :] = img_lab_outres[..., 0]\n",
        "\n",
        "        color_ab[0, :, :] = img_lab[..., 1].reshape(1, self.shape[0], self.shape[1])\n",
        "        color_ab[1, :, :] = img_lab[..., 2].reshape(1, self.shape[0], self.shape[1])\n",
        "\n",
        "        if self.lossweights is not None:\n",
        "            weights = self.__getweights__(color_ab)\n",
        "\n",
        "        # Load feature maps\n",
        "        featobj = np.load(self.feats_fns[idx])\n",
        "        greyfeats[:, :, :] = featobj[\"arr_0\"]\n",
        "\n",
        "        return color_ab, recon_const, weights, recon_const_outres, greyfeats\n",
        "\n",
        "    def __getweights__(self, img):\n",
        "        \"\"\"\n",
        "        Calculate weight values for each pixel of an image.\n",
        "        \"\"\"\n",
        "        img_vec = img.reshape(-1)\n",
        "        img_vec = img_vec * 128.0\n",
        "        img_lossweights = np.zeros(img.shape, dtype=\"f\")\n",
        "        img_vec_a = img_vec[: np.prod(self.shape)]\n",
        "        binedges_a = self.binedges[0, ...].reshape(-1)\n",
        "        binid_a = [binedges_a.flat[np.abs(binedges_a - v).argmin()] for v in img_vec_a]\n",
        "        img_vec_b = img_vec[np.prod(self.shape) :]\n",
        "        binedges_b = self.binedges[1, ...].reshape(-1)\n",
        "        binid_b = [binedges_b.flat[np.abs(binedges_b - v).argmin()] for v in img_vec_b]\n",
        "        binweights = np.array([self.lossweights[v1][v2] for v1, v2 in zip(binid_a, binid_b)])\n",
        "        img_lossweights[0, :, :] = binweights.reshape(self.shape[0], self.shape[1])\n",
        "        img_lossweights[1, :, :] = binweights.reshape(self.shape[0], self.shape[1])\n",
        "        return img_lossweights\n",
        "\n",
        "    def saveoutput_gt(self, net_op, gt, prefix, batch_size, num_cols=8, net_recon_const=None):\n",
        "        \"\"\"\n",
        "        Save images\n",
        "        \"\"\"\n",
        "        net_out_img = self.__tiledoutput__(net_op, batch_size, num_cols=num_cols, net_recon_const=net_recon_const)\n",
        "        gt_out_img = self.__tiledoutput__(gt, batch_size, num_cols=num_cols, net_recon_const=net_recon_const)\n",
        "\n",
        "        num_rows = np.int_(np.ceil((batch_size * 1.0) / num_cols))\n",
        "        border_img = 255 * np.ones((num_rows * self.outshape[0], 128, 3), dtype=\"uint8\")\n",
        "        out_fn_pred = \"%s/%s.png\" % (self.out_directory, prefix)\n",
        "        cv2.imwrite(out_fn_pred, np.concatenate((net_out_img, border_img, gt_out_img), axis=1))\n",
        "\n",
        "    def __tiledoutput__(self, net_op, batch_size, num_cols=8, net_recon_const=None):\n",
        "        \"\"\"\n",
        "        Generate a combined image from these inputs by stitching the images into a large image.\n",
        "        \"\"\"\n",
        "        num_rows = np.int_(np.ceil((batch_size * 1.0) / num_cols))\n",
        "        out_img = np.zeros((num_rows * self.outshape[0], num_cols * self.outshape[1], 3), dtype=\"uint8\")\n",
        "        img_lab = np.zeros((self.outshape[0], self.outshape[1], 3), dtype=\"uint8\")\n",
        "        c = 0\n",
        "        r = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            if i % num_cols == 0 and i > 0:\n",
        "                r = r + 1\n",
        "                c = 0\n",
        "            img_lab[..., 0] = self.__decodeimg__(net_recon_const[i, 0, :, :].reshape(self.outshape[0], self.outshape[1]))\n",
        "            img_lab[..., 1] = self.__decodeimg__(net_op[i, 0, :, :].reshape(self.shape[0], self.shape[1]))\n",
        "            img_lab[..., 2] = self.__decodeimg__(net_op[i, 1, :, :].reshape(self.shape[0], self.shape[1]))\n",
        "            img_rgb = cv2.cvtColor(img_lab, cv2.COLOR_LAB2BGR)\n",
        "            out_img[\n",
        "                r * self.outshape[0] : (r + 1) * self.outshape[0],\n",
        "                c * self.outshape[1] : (c + 1) * self.outshape[1],\n",
        "                ...,\n",
        "            ] = img_rgb\n",
        "            c = c + 1\n",
        "\n",
        "        return out_img\n",
        "\n",
        "    def __decodeimg__(self, img_enc):\n",
        "        \"\"\"\n",
        "        Denormalize from [-1..1] to [0..255]\n",
        "        \"\"\"\n",
        "        img_dec = (((img_enc + 1.0) * 1.0) / 2.0) * 255.0\n",
        "        img_dec[img_dec < 0.0] = 0.0\n",
        "        img_dec[img_dec > 255.0] = 255.0\n",
        "        return cv2.resize(np.uint8(img_dec), (self.outshape[0], self.outshape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare hyperparameters\n",
        "args = {\n",
        "    \"gpu\": 1,\n",
        "    \"epochs\": 10,\n",
        "    \"epochs_mdn\": 10,\n",
        "    \"batchsize\": 32,\n",
        "    \"hiddensize\": 64,\n",
        "    \"nthreads\": 2,\n",
        "    \"nmix\": 8,\n",
        "    \"logstep\": 100,\n",
        "    \"dataset_key\": \"lfw\"\n",
        "}\n",
        "\n",
        "def get_dirpaths(args):\n",
        "    if args[\"dataset_key\"] == \"lfw\":\n",
        "        out_dir = \"data/output/lfw\"\n",
        "        listdir = \"data/imglist/lfw\"\n",
        "        featslistdir = \"data/featslist/lfw\"\n",
        "    else:\n",
        "        raise NameError(\"[ERROR] Incorrect key: %s\" % (args.dataset_key))\n",
        "    return out_dir, listdir, featslistdir"
      ],
      "metadata": {
        "id": "Sh0xPJvmk_O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Build models"
      ],
      "metadata": {
        "id": "CaKEQ0a4IRkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Build VAE model"
      ],
      "metadata": {
        "id": "UM14XUb4g1gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.hidden_size = 64\n",
        "\n",
        "        # Encoder block - (2, 64, 64)\n",
        "        self.enc_conv1 = nn.Conv2d(2, 128, 5, stride=2, padding=2)      # (128, 32, 32)\n",
        "        self.enc_bn1 = nn.BatchNorm2d(128)\n",
        "        self.enc_conv2 = nn.Conv2d(128, 256, 5, stride=2, padding=2)    # (256, 16, 16)\n",
        "        self.enc_bn2 = nn.BatchNorm2d(256)\n",
        "        self.enc_conv3 = nn.Conv2d(256, 512, 5, stride=2, padding=2)    # (512, 8, 8)\n",
        "        self.enc_bn3 = nn.BatchNorm2d(512)\n",
        "        self.enc_conv4 = nn.Conv2d(512, 1024, 3, stride=2, padding=1)   # (1024, 4, 4)\n",
        "        self.enc_bn4 = nn.BatchNorm2d(1024)\n",
        "        self.enc_fc1 = nn.Linear(4*4*1024, self.hidden_size*2)          # (128,)\n",
        "        self.enc_dropout1 = nn.Dropout(p=0.7)\n",
        "\n",
        "        # Conditional encoder block - (1, 64, 64)\n",
        "        self.cond_enc_conv1 = nn.Conv2d(1, 128, 5, stride=2, padding=2)     # (128, 32, 32)\n",
        "        self.cond_enc_bn1 = nn.BatchNorm2d(128)\n",
        "        self.cond_enc_conv2 = nn.Conv2d(128, 256, 5, stride=2, padding=2)   # (256, 16, 16)\n",
        "        self.cond_enc_bn2 = nn.BatchNorm2d(256)\n",
        "        self.cond_enc_conv3 = nn.Conv2d(256, 512, 5, stride=2, padding=2)   # (512, 8, 8)\n",
        "        self.cond_enc_bn3 = nn.BatchNorm2d(512)\n",
        "        self.cond_enc_conv4 = nn.Conv2d(512, 1024, 3, stride=2, padding=1)  # (1024, 4, 4)\n",
        "        self.cond_enc_bn4 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        # Decoder block - (64, 1, 1)\n",
        "        self.dec_upsamp1 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
        "        self.dec_conv1 = nn.Conv2d(1024+self.hidden_size, 512, 3, stride=1, padding=1)\n",
        "        self.dec_bn1 = nn.BatchNorm2d(512)\n",
        "        self.dec_upsamp2 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.dec_conv2 = nn.Conv2d(512*2, 256, 5, stride=1, padding=2)\n",
        "        self.dec_bn2 = nn.BatchNorm2d(256)\n",
        "        self.dec_upsamp3 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.dec_conv3 = nn.Conv2d(256*2, 128, 5, stride=1, padding=2)\n",
        "        self.dec_bn3 = nn.BatchNorm2d(128)\n",
        "        self.dec_upsamp4 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.dec_conv4 = nn.Conv2d(128*2, 64, 5, stride=1, padding=2)\n",
        "        self.dec_bn4 = nn.BatchNorm2d(64)\n",
        "        self.dec_upsamp5 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.dec_conv5 = nn.Conv2d(64, 2, 5, stride=1, padding=2)\n",
        "\n",
        "    def encoder(self, x):                   # (2, 64, 64)\n",
        "        x = F.relu(self.enc_conv1(x))\n",
        "        x = self.enc_bn1(x)                 # (128, 32, 32)\n",
        "        x = F.relu(self.enc_conv2(x))\n",
        "        x = self.enc_bn2(x)                 # (256, 16, 16)\n",
        "        x = F.relu(self.enc_conv3(x))\n",
        "        x = self.enc_bn3(x)                 # (512, 8, 8)\n",
        "        x = F.relu(self.enc_conv4(x))\n",
        "        x = self.enc_bn4(x)                 # (1024, 4, 4)\n",
        "        x = x.view(-1, 4*4*1024)\n",
        "        x = self.enc_dropout1(x)\n",
        "        x = self.enc_fc1(x)                 # (128,)\n",
        "        mu = x[..., :self.hidden_size]      # (64,)\n",
        "        logvar = x[..., self.hidden_size:]  # (64,)\n",
        "        return mu, logvar\n",
        "\n",
        "    def cond_encoder(self, x):                      # (1, 64, 64)\n",
        "        x = F.relu(self.cond_enc_conv1(x))\n",
        "        sc_feat32 = self.cond_enc_bn1(x)            # (128, 32, 32)\n",
        "        x = F.relu(self.cond_enc_conv2(sc_feat32))\n",
        "        sc_feat16 = self.cond_enc_bn2(x)            # (256, 16, 16)\n",
        "        x = F.relu(self.cond_enc_conv3(sc_feat16))\n",
        "        sc_feat8 = self.cond_enc_bn3(x)             # (512, 8, 8)\n",
        "        x = F.relu(self.cond_enc_conv4(sc_feat8))\n",
        "        sc_feat4 = self.cond_enc_bn4(x)             # (1024, 4, 4)\n",
        "        return sc_feat32, sc_feat16, sc_feat8, sc_feat4\n",
        "\n",
        "    def decoder(self, z, sc_feat32, sc_feat16, sc_feat8, sc_feat4):\n",
        "        x = z.view(-1, self.hidden_size, 1, 1)      # (64, 1, 1)\n",
        "        x = self.dec_upsamp1(x)                     # (64, 4, 4)\n",
        "        x = torch.cat([x, sc_feat4], 1)             # (64+1024, 4, 4)\n",
        "        x = F.relu(self.dec_conv1(x))               # (512, 4, 4)\n",
        "        x = self.dec_bn1(x)                         # (512, 4, 4)\n",
        "        x = self.dec_upsamp2(x)                     # (512, 8, 8)\n",
        "        x = torch.cat([x, sc_feat8], 1)             # (512+512, 8, 8)\n",
        "        x = F.relu(self.dec_conv2(x))               # (256, 8, 8)\n",
        "        x = self.dec_bn2(x)                         # (256, 8, 8)\n",
        "        x = self.dec_upsamp3(x)                     # (256, 16, 16)\n",
        "        x = torch.cat([x, sc_feat16], 1)            # (256+256, 16, 16)\n",
        "        x = F.relu(self.dec_conv3(x))               # (128, 16, 16)\n",
        "        x = self.dec_bn3(x)                         # (128, 16, 16)\n",
        "        x = self.dec_upsamp4(x)                     # (128, 32, 32)\n",
        "        x = torch.cat([x, sc_feat32], 1)            # (128+128, 32, 32)\n",
        "        x = F.relu(self.dec_conv4(x))               # (64, 32, 32)\n",
        "        x = self.dec_bn4(x)                         # (64, 32, 32)\n",
        "        x = self.dec_upsamp5(x)                     # (64, 64, 64)\n",
        "        x = torch.tanh(self.dec_conv5(x))           # (2, 64, 64)\n",
        "        return x\n",
        "\n",
        "    def forward(self, color, greylevel, z_in=None):\n",
        "        sc_feat32, sc_feat16, sc_feat8, sc_feat4 = self.cond_encoder(greylevel)\n",
        "        mu, logvar = self.encoder(color)\n",
        "        if self.training:\n",
        "            stddev = torch.sqrt(torch.exp(logvar))\n",
        "            eps = torch.randn_like(stddev)\n",
        "            z = mu + eps * stddev\n",
        "            z = z.to(greylevel.device)\n",
        "        else:\n",
        "            z = z_in\n",
        "            z = z.to(greylevel.device)\n",
        "        color_out = self.decoder(z, sc_feat32, sc_feat16, sc_feat8, sc_feat4)\n",
        "        return mu, logvar, color_out"
      ],
      "metadata": {
        "id": "m3gfw1k5fx-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "temp_model = VAE()\n",
        "summary(temp_model, [(2, 64, 64), (1, 64, 64), (64, 1, 1)], device=\"cpu\")"
      ],
      "metadata": {
        "id": "XuJ7CMX6hfqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98669699-989a-4962-9827-2915b4592417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 128, 32, 32]           3,328\n",
            "       BatchNorm2d-2          [-1, 128, 32, 32]             256\n",
            "            Conv2d-3          [-1, 256, 16, 16]         819,456\n",
            "       BatchNorm2d-4          [-1, 256, 16, 16]             512\n",
            "            Conv2d-5            [-1, 512, 8, 8]       3,277,312\n",
            "       BatchNorm2d-6            [-1, 512, 8, 8]           1,024\n",
            "            Conv2d-7           [-1, 1024, 4, 4]       4,719,616\n",
            "       BatchNorm2d-8           [-1, 1024, 4, 4]           2,048\n",
            "            Conv2d-9          [-1, 128, 32, 32]           6,528\n",
            "      BatchNorm2d-10          [-1, 128, 32, 32]             256\n",
            "           Conv2d-11          [-1, 256, 16, 16]         819,456\n",
            "      BatchNorm2d-12          [-1, 256, 16, 16]             512\n",
            "           Conv2d-13            [-1, 512, 8, 8]       3,277,312\n",
            "      BatchNorm2d-14            [-1, 512, 8, 8]           1,024\n",
            "           Conv2d-15           [-1, 1024, 4, 4]       4,719,616\n",
            "      BatchNorm2d-16           [-1, 1024, 4, 4]           2,048\n",
            "          Dropout-17                [-1, 16384]               0\n",
            "           Linear-18                  [-1, 128]       2,097,280\n",
            "         Upsample-19             [-1, 64, 4, 4]               0\n",
            "           Conv2d-20            [-1, 512, 4, 4]       5,014,016\n",
            "      BatchNorm2d-21            [-1, 512, 4, 4]           1,024\n",
            "         Upsample-22            [-1, 512, 8, 8]               0\n",
            "           Conv2d-23            [-1, 256, 8, 8]       6,553,856\n",
            "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
            "         Upsample-25          [-1, 256, 16, 16]               0\n",
            "           Conv2d-26          [-1, 128, 16, 16]       1,638,528\n",
            "      BatchNorm2d-27          [-1, 128, 16, 16]             256\n",
            "         Upsample-28          [-1, 128, 32, 32]               0\n",
            "           Conv2d-29           [-1, 64, 32, 32]         409,664\n",
            "      BatchNorm2d-30           [-1, 64, 32, 32]             128\n",
            "         Upsample-31           [-1, 64, 64, 64]               0\n",
            "           Conv2d-32            [-1, 2, 64, 64]           3,202\n",
            "================================================================\n",
            "Total params: 33,368,770\n",
            "Trainable params: 33,368,770\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 8192.00\n",
            "Forward/backward pass size (MB): 13.32\n",
            "Params size (MB): 127.29\n",
            "Estimated Total Size (MB): 8332.61\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Build MDN model"
      ],
      "metadata": {
        "id": "7y2lgmpVi0eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MDN, self).__init__()\n",
        "\n",
        "        self.feats_nch = 512\n",
        "        self.hidden_size = 64\n",
        "        self.nmix = 8\n",
        "        self.nout = (self.hidden_size + 1) * self.nmix\n",
        "\n",
        "        # Define MDN Layers - (512, 64, 64)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(self.feats_nch, 384, 5, stride=1, padding=2), # (384, 28, 28)\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 320, 5, stride=1, padding=2),            # (320, 28, 28)\n",
        "            nn.BatchNorm2d(320),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(320, 288, 5, stride=1, padding=2),            # (288, 28, 28)\n",
        "            nn.BatchNorm2d(288),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(288, 256, 5, stride=2, padding=2),            # (256, 14, 14)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 128, 5, stride=1, padding=2),            # (128, 14, 14)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 96, 5, stride=2, padding=2),             # (96, 7, 7)\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(96, 64, 5, stride=2, padding=2),              # (64, 4, 4)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.7)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(4 * 4 * 64, self.nout)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        x = self.model(feats)\n",
        "        x = x.view(-1, 4 * 4 * 64)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.7, training=self.training)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "aiC5aWRrizTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "model = MDN()\n",
        "summary(model, (512, 28, 28), device=\"cpu\")   # (520,)"
      ],
      "metadata": {
        "id": "xhMvcd6Hi8E_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1d9cd3-35c0-4ba1-ff3f-92bbf5f151cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 384, 28, 28]       4,915,584\n",
            "       BatchNorm2d-2          [-1, 384, 28, 28]             768\n",
            "              ReLU-3          [-1, 384, 28, 28]               0\n",
            "            Conv2d-4          [-1, 320, 28, 28]       3,072,320\n",
            "       BatchNorm2d-5          [-1, 320, 28, 28]             640\n",
            "              ReLU-6          [-1, 320, 28, 28]               0\n",
            "            Conv2d-7          [-1, 288, 28, 28]       2,304,288\n",
            "       BatchNorm2d-8          [-1, 288, 28, 28]             576\n",
            "              ReLU-9          [-1, 288, 28, 28]               0\n",
            "           Conv2d-10          [-1, 256, 14, 14]       1,843,456\n",
            "      BatchNorm2d-11          [-1, 256, 14, 14]             512\n",
            "             ReLU-12          [-1, 256, 14, 14]               0\n",
            "           Conv2d-13          [-1, 128, 14, 14]         819,328\n",
            "      BatchNorm2d-14          [-1, 128, 14, 14]             256\n",
            "             ReLU-15          [-1, 128, 14, 14]               0\n",
            "           Conv2d-16             [-1, 96, 7, 7]         307,296\n",
            "      BatchNorm2d-17             [-1, 96, 7, 7]             192\n",
            "             ReLU-18             [-1, 96, 7, 7]               0\n",
            "           Conv2d-19             [-1, 64, 4, 4]         153,664\n",
            "      BatchNorm2d-20             [-1, 64, 4, 4]             128\n",
            "             ReLU-21             [-1, 64, 4, 4]               0\n",
            "          Dropout-22             [-1, 64, 4, 4]               0\n",
            "           Linear-23                  [-1, 520]         533,000\n",
            "================================================================\n",
            "Total params: 13,952,008\n",
            "Trainable params: 13,952,008\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.53\n",
            "Forward/backward pass size (MB): 19.67\n",
            "Params size (MB): 53.22\n",
            "Estimated Total Size (MB): 74.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Loss functions"
      ],
      "metadata": {
        "id": "GnphuygzjsPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. VAE Loss"
      ],
      "metadata": {
        "id": "Wb2N--_3lXpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(mu, logvar, pred, gt, lossweights, batchsize):\n",
        "    '''\n",
        "    Return the loss values of the VAE model.\n",
        "    '''\n",
        "    kl_element = torch.add(torch.add(torch.add(mu.pow(2), logvar.exp()), -1), logvar.mul(-1))\n",
        "    kl_loss = torch.sum(kl_element).mul(0.5)\n",
        "    gt = gt.reshape(-1, 64 * 64 * 2)\n",
        "    pred = pred.reshape(-1, 64 * 64 * 2)\n",
        "    recon_element = torch.sqrt(torch.sum(torch.mul(torch.add(gt, pred.mul(-1)).pow(2), lossweights), 1))\n",
        "    recon_loss = torch.sum(recon_element).mul(1.0 / (batchsize))\n",
        "\n",
        "    recon_element_l2 = torch.sqrt(torch.sum(torch.add(gt, pred.mul(-1)).pow(2), 1))\n",
        "    recon_loss_l2 = torch.sum(recon_element_l2).mul(1.0 / (batchsize))\n",
        "\n",
        "    return kl_loss, recon_loss, recon_loss_l2"
      ],
      "metadata": {
        "id": "BK9pfWv2lVQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. MDN Loss"
      ],
      "metadata": {
        "id": "TKwHUSkNld42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gmm_coeffs(gmm_params):\n",
        "    \"\"\"\n",
        "    Return the distribution coefficients of the GMM.\n",
        "    \"\"\"\n",
        "    gmm_mu = gmm_params[..., : args[\"hiddensize\"] * args[\"nmix\"]]\n",
        "    gmm_mu.contiguous()\n",
        "    gmm_pi_activ = gmm_params[..., args[\"hiddensize\"] * args[\"nmix\"] :]\n",
        "    gmm_pi_activ.contiguous()\n",
        "    gmm_pi = F.softmax(gmm_pi_activ, dim=1)\n",
        "    return gmm_mu, gmm_pi\n",
        "\n",
        "def mdn_loss(gmm_params, mu, stddev, batchsize):\n",
        "    \"\"\"\n",
        "    Calculates the loss by comparing two distribution\n",
        "    - the predicted distribution of the MDN (given by gmm_mu and gmm_pi) with\n",
        "    - the target distribution created by the Encoder block (given by mu and stddev).\n",
        "    \"\"\"\n",
        "    gmm_mu, gmm_pi = get_gmm_coeffs(gmm_params)\n",
        "    eps = torch.randn(stddev.size()).normal_().cuda()\n",
        "    z = torch.add(mu, torch.mul(eps, stddev))\n",
        "    z_flat = z.repeat(1, args[\"nmix\"])\n",
        "    z_flat = z_flat.reshape(batchsize * args[\"nmix\"], args[\"hiddensize\"])\n",
        "    gmm_mu_flat = gmm_mu.reshape(batchsize * args[\"nmix\"], args[\"hiddensize\"])\n",
        "    dist_all = torch.sqrt(torch.sum(torch.add(z_flat, gmm_mu_flat.mul(-1)).pow(2).mul(50), 1))\n",
        "    dist_all = dist_all.reshape(batchsize, args[\"nmix\"])\n",
        "    dist_min, selectids = torch.min(dist_all, 1)\n",
        "    gmm_pi_min = torch.gather(gmm_pi, 1, selectids.reshape(-1, 1))\n",
        "    gmm_loss = torch.mean(torch.add(-1 * torch.log(gmm_pi_min + 1e-30), dist_min))\n",
        "    gmm_loss_l2 = torch.mean(dist_min)\n",
        "    return gmm_loss, gmm_loss_l2"
      ],
      "metadata": {
        "id": "v4HroyIkjul1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train models"
      ],
      "metadata": {
        "id": "cp-dPOAPIo2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Train VAE model"
      ],
      "metadata": {
        "id": "DK-gq00-lFle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_vae(model):\n",
        "    model.eval()\n",
        "\n",
        "    # Load hyperparameters\n",
        "    out_dir, listdir, featslistdir = get_dirpaths(args)\n",
        "    batchsize = args[\"batchsize\"]\n",
        "    hiddensize = args[\"hiddensize\"]\n",
        "    nmix = args[\"nmix\"]\n",
        "\n",
        "    # Create DataLoader\n",
        "    data = ColorDataset(os.path.join(out_dir, \"images\"), listdir, featslistdir, split=\"test\")\n",
        "    nbatches = np.int_(np.floor(data.img_num / batchsize))\n",
        "    data_loader = DataLoader(dataset=data, num_workers=args[\"nthreads\"], batch_size=batchsize, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Eval\n",
        "    test_loss = 0.0\n",
        "    for batch_idx, (batch, batch_recon_const, batch_weights, batch_recon_const_outres, _) in tqdm(enumerate(data_loader), total=nbatches):\n",
        "        input_color = batch.cuda()\n",
        "        lossweights = batch_weights.cuda()\n",
        "        lossweights = lossweights.reshape(batchsize, -1)\n",
        "        input_greylevel = batch_recon_const.cuda()\n",
        "        z = torch.randn(batchsize, hiddensize)\n",
        "\n",
        "        mu, logvar, color_out = model(input_color, input_greylevel, z)\n",
        "        _, _, recon_loss_l2 = vae_loss(mu, logvar, color_out, input_color, lossweights, batchsize)\n",
        "        test_loss = test_loss + recon_loss_l2.item()\n",
        "\n",
        "    test_loss = (test_loss * 1.0) / nbatches\n",
        "    model.train()\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "zUoGXfPu2KjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae():\n",
        "    # Load hyperparameters\n",
        "    out_dir, listdir, featslistdir = get_dirpaths(args)\n",
        "    batchsize = args[\"batchsize\"]\n",
        "    hiddensize = args[\"hiddensize\"]\n",
        "    nmix = args[\"nmix\"]\n",
        "    nepochs = args[\"epochs\"]\n",
        "\n",
        "    # Create DataLoader\n",
        "    data = ColorDataset(os.path.join(out_dir, \"images\"), listdir, featslistdir, split=\"train\")\n",
        "    nbatches = np.int_(np.floor(data.img_num / batchsize))\n",
        "    data_loader = DataLoader(dataset=data, num_workers=args[\"nthreads\"], batch_size=batchsize, shuffle=True, drop_last=True)\n",
        "\n",
        "    # Initialize VAE model\n",
        "    model = VAE()\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Train\n",
        "    itr_idx = 0\n",
        "    for epochs in range(nepochs):\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (batch, batch_recon_const, batch_weights, batch_recon_const_outres, _) in tqdm(enumerate(data_loader), total=nbatches):\n",
        "            input_color = batch.cuda()\n",
        "            lossweights = batch_weights.cuda()\n",
        "            lossweights = lossweights.reshape(batchsize, -1)\n",
        "            input_greylevel = batch_recon_const.cuda()\n",
        "            z = torch.randn(batchsize, hiddensize)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            mu, logvar, color_out = model(input_color, input_greylevel, z)\n",
        "            kl_loss, recon_loss, recon_loss_l2 = vae_loss(mu, logvar, color_out, input_color, lossweights, batchsize)\n",
        "            loss = kl_loss.mul(1e-2) + recon_loss\n",
        "            recon_loss_l2.detach()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss = train_loss + recon_loss_l2.item()\n",
        "\n",
        "            if batch_idx % args[\"logstep\"] == 0:\n",
        "                data.saveoutput_gt(\n",
        "                    color_out.cpu().data.numpy(),\n",
        "                    batch.numpy(),\n",
        "                    \"train_%05d_%05d\" % (epochs, batch_idx),\n",
        "                    batchsize,\n",
        "                    net_recon_const=batch_recon_const_outres.numpy()\n",
        "                )\n",
        "\n",
        "        train_loss = (train_loss * 1.0) / (nbatches)\n",
        "        test_loss = test_vae(model)\n",
        "        print(f\"End of epoch {epochs:3d} | Train Loss {train_loss:8.3f} | Test Loss {test_loss:8.3f} \")\n",
        "\n",
        "        # Save VAE model\n",
        "        torch.save(model.state_dict(), \"%s/models/model_vae.pth\" % (out_dir))\n",
        "\n",
        "    print(\"Complete VAE training\")"
      ],
      "metadata": {
        "id": "0cTlh5N6jlEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vae()"
      ],
      "metadata": {
        "id": "m-V4Soazl6GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Train MDN model"
      ],
      "metadata": {
        "id": "9KGxIpG7l8Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mdn(model_vae, model_mdn):\n",
        "    # Load hyperparameters\n",
        "    out_dir, listdir, featslistdir = get_dirpaths(args)\n",
        "    batchsize = args[\"batchsize\"]\n",
        "    hiddensize = args[\"hiddensize\"]\n",
        "    nmix = args[\"nmix\"]\n",
        "\n",
        "    # Create DataLoader\n",
        "    data = ColorDataset(os.path.join(out_dir, \"images\"), listdir, featslistdir, split=\"test\")\n",
        "    nbatches = np.int_(np.floor(data.img_num / batchsize))\n",
        "    data_loader = DataLoader(dataset=data, num_workers=args[\"nthreads\"], batch_size=batchsize, shuffle=True, drop_last=True)\n",
        "\n",
        "    optimizer = optim.Adam(model_mdn.parameters(), lr=1e-3)\n",
        "\n",
        "    # Eval\n",
        "    model_vae.eval()\n",
        "    model_mdn.eval()\n",
        "    itr_idx = 0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    for batch_idx, (batch, batch_recon_const, batch_weights, _, batch_feats) in tqdm(enumerate(data_loader), total=nbatches):\n",
        "        input_color = batch.cuda()\n",
        "        input_greylevel = batch_recon_const.cuda()\n",
        "        input_feats = batch_feats.cuda()\n",
        "        z = torch.randn(batchsize, hiddensize)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the parameters of the posterior distribution\n",
        "        mu, logvar, _ = model_vae(input_color, input_greylevel, z)\n",
        "\n",
        "        # Get the GMM vector\n",
        "        mdn_gmm_params = model_mdn(input_feats)\n",
        "\n",
        "        # Compare 2 distributions\n",
        "        loss, _ = mdn_loss(mdn_gmm_params, mu, torch.sqrt(torch.exp(logvar)), batchsize)\n",
        "\n",
        "\n",
        "        test_loss = test_loss + loss.item()\n",
        "\n",
        "    test_loss = (test_loss * 1.0) / (nbatches)\n",
        "    model_vae.train()\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "eJ65LPgZ08Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mdn():\n",
        "    # Load hyperparameters\n",
        "    out_dir, listdir, featslistdir = get_dirpaths(args)\n",
        "    batchsize = args[\"batchsize\"]\n",
        "    hiddensize = args[\"hiddensize\"]\n",
        "    nmix = args[\"nmix\"]\n",
        "    nepochs = args[\"epochs_mdn\"]\n",
        "\n",
        "    # Create DataLoader\n",
        "    data = ColorDataset(os.path.join(out_dir, \"images\"), listdir, featslistdir, split=\"train\")\n",
        "    nbatches = np.int_(np.floor(data.img_num / batchsize))\n",
        "    data_loader = DataLoader(dataset=data, num_workers=args[\"nthreads\"], batch_size=batchsize, shuffle=True, drop_last=True)\n",
        "\n",
        "    # Initialize VAE model\n",
        "    model_vae = VAE()\n",
        "    model_vae.cuda()\n",
        "    model_vae.load_state_dict(torch.load(\"%s/models/model_vae.pth\" % (out_dir)))\n",
        "    model_vae.eval()\n",
        "\n",
        "    # Initialize MDN model\n",
        "    model_mdn = MDN()\n",
        "    model_mdn.cuda()\n",
        "    model_mdn.train()\n",
        "\n",
        "    optimizer = optim.Adam(model_mdn.parameters(), lr=1e-3)\n",
        "\n",
        "    # Train\n",
        "    itr_idx = 0\n",
        "    for epochs_mdn in range(nepochs):\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (batch, batch_recon_const, batch_weights, _, batch_feats) in tqdm(enumerate(data_loader), total=nbatches):\n",
        "            input_color = batch.cuda()\n",
        "            input_greylevel = batch_recon_const.cuda()\n",
        "            input_feats = batch_feats.cuda()\n",
        "            z = torch.randn(batchsize, hiddensize)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get the parameters of the posterior distribution\n",
        "            mu, logvar, _ = model_vae(input_color, input_greylevel, z)\n",
        "\n",
        "            # Get the GMM vector\n",
        "            mdn_gmm_params = model_mdn(input_feats)\n",
        "\n",
        "            # Compare 2 distributions\n",
        "            loss, loss_l2 = mdn_loss(mdn_gmm_params, mu, torch.sqrt(torch.exp(logvar)), batchsize)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss = train_loss + loss.item()\n",
        "\n",
        "        train_loss = (train_loss * 1.0) / (nbatches)\n",
        "        test_loss = test_mdn(model_vae, model_mdn)\n",
        "        print(f\"End of epoch {epochs_mdn:3d} | Train Loss {train_loss:8.3f} |  Test Loss {test_loss:8.3f}\")\n",
        "\n",
        "        # Save MDN model\n",
        "        torch.save(model_mdn.state_dict(), \"%s/models_mdn/model_mdn.pth\" % (out_dir))\n",
        "\n",
        "    print(\"Complete MDN training\")"
      ],
      "metadata": {
        "id": "Y5KRvdh0lm-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mdn()"
      ],
      "metadata": {
        "id": "gDYCeEWkmFon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Inference"
      ],
      "metadata": {
        "id": "HXU0ihSSl_DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(vae_ckpt=None, mdn_ckpt=None):\n",
        "    # Load hyperparameters\n",
        "    out_dir, listdir, featslistdir = get_dirpaths(args)\n",
        "    batchsize = args[\"batchsize\"]\n",
        "    hiddensize = args[\"hiddensize\"]\n",
        "    nmix = args[\"nmix\"]\n",
        "\n",
        "    # Create DataLoader\n",
        "    data = ColorDataset(os.path.join(out_dir, \"images\"), listdir, featslistdir, split=\"test\")\n",
        "    nbatches = np.int_(np.floor(data.img_num / batchsize))\n",
        "    data_loader = DataLoader(dataset=data, num_workers=args[\"nthreads\"], batch_size=batchsize, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Initialize VAE model\n",
        "    model_vae = VAE()\n",
        "    model_vae.cuda()\n",
        "    if vae_ckpt:\n",
        "        model_vae.load_state_dict(torch.load(vae_ckpt))\n",
        "    else:\n",
        "        model_vae.load_state_dict(torch.load(\"%s/models/model_vae.pth\" % (out_dir)))\n",
        "    model_vae.eval()\n",
        "\n",
        "    # Initialize MDN model\n",
        "    model_mdn = MDN()\n",
        "    model_mdn.cuda()\n",
        "    if mdn_ckpt:\n",
        "        model_mdn.load_state_dict(torch.load(mdn_ckpt))\n",
        "    else:\n",
        "        model_mdn.load_state_dict(torch.load(\"%s/models_mdn/model_mdn.pth\" % (out_dir)))\n",
        "    model_mdn.eval()\n",
        "\n",
        "    # Infer\n",
        "    for batch_idx, (batch, batch_recon_const, batch_weights, batch_recon_const_outres, batch_feats) in tqdm(enumerate(data_loader), total=nbatches):\n",
        "        input_feats = batch_feats.cuda()\n",
        "\n",
        "        # Get GMM parameters\n",
        "        mdn_gmm_params = model_mdn(input_feats)\n",
        "        gmm_mu, gmm_pi = get_gmm_coeffs(mdn_gmm_params)\n",
        "        gmm_pi = gmm_pi.reshape(-1, 1)\n",
        "        gmm_mu = gmm_mu.reshape(-1, hiddensize)\n",
        "\n",
        "        for j in range(batchsize):\n",
        "            batch_j = np.tile(batch[j, ...].numpy(), (batchsize, 1, 1, 1))\n",
        "            batch_recon_const_j = np.tile(batch_recon_const[j, ...].numpy(), (batchsize, 1, 1, 1))\n",
        "            batch_recon_const_outres_j = np.tile(batch_recon_const_outres[j, ...].numpy(), (batchsize, 1, 1, 1))\n",
        "\n",
        "            input_color = torch.from_numpy(batch_j).cuda()\n",
        "            input_greylevel = torch.from_numpy(batch_recon_const_j).cuda()\n",
        "\n",
        "            # Get mean from GMM\n",
        "            curr_mu = gmm_mu[j * nmix : (j + 1) * nmix, :]\n",
        "            orderid = np.argsort(gmm_pi[j * nmix : (j + 1) * nmix, 0].cpu().data.numpy().reshape(-1))\n",
        "\n",
        "            # Sample from GMM\n",
        "            z = curr_mu.repeat(int((batchsize * 1.0) / nmix), 1)\n",
        "\n",
        "            # Predict color\n",
        "            _, _, color_out = model_vae(input_color, input_greylevel, z)\n",
        "\n",
        "            data.saveoutput_gt(\n",
        "                color_out.cpu().data.numpy()[orderid, ...],\n",
        "                batch_j[orderid, ...],\n",
        "                \"divcolor_%05d_%05d\" % (batch_idx, j),\n",
        "                nmix,\n",
        "                net_recon_const=batch_recon_const_outres_j[orderid, ...],\n",
        "            )\n",
        "\n",
        "    print(\"Complete inference. The results are saved in data/output/lfw/images.\")"
      ],
      "metadata": {
        "id": "ynhp7Nzql2fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download VAE checkpoint\n",
        "!gdown 1wdyK198lXwwZO4NIB7DzJmA5arwUVWDU\n",
        "# Download MDN checkpoint\n",
        "!gdown 1AhilMrR_C04v7_sysuf5ffEVsQllo2W6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy1iCkJ0lzCa",
        "outputId": "35f467c9-ad2e-43dd-a35a-699f96f55764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wdyK198lXwwZO4NIB7DzJmA5arwUVWDU\n",
            "From (redirected): https://drive.google.com/uc?id=1wdyK198lXwwZO4NIB7DzJmA5arwUVWDU&confirm=t&uuid=4016ad4c-eda8-414e-a90b-143d042f305d\n",
            "To: /content/model_vae.pth\n",
            "100% 134M/134M [00:02<00:00, 54.7MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1AhilMrR_C04v7_sysuf5ffEVsQllo2W6\n",
            "From (redirected): https://drive.google.com/uc?id=1AhilMrR_C04v7_sysuf5ffEVsQllo2W6&confirm=t&uuid=5a32c430-e17a-44d8-b54f-2b6cf63f0a6c\n",
            "To: /content/model_mdn.pth\n",
            "100% 55.8M/55.8M [00:01<00:00, 41.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae_ckpt = \"model_vae.pth\"\n",
        "mdn_ckpt = \"model_mdn.pth\"\n",
        "inference(vae_ckpt, mdn_ckpt)"
      ],
      "metadata": {
        "id": "iMmzlCf6mIsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc03c07-1d93-4559-e5b8-0d9f4cce3a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:10<00:00,  5.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete inference\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Reference**\n",
        "\n",
        "1. Learning Diverse Image Colorization - [Paper](https://arxiv.org/pdf/1612.01958.pdf) - [Official code](https://github.com/aditya12agd5/divcolor)\n",
        "\n",
        "2. Colorful Image Colorization - [Paper](https://arxiv.org/pdf/1603.08511.pdf) - [Official code](https://github.com/richzhang/colorization)"
      ],
      "metadata": {
        "id": "ZpZgWKcQJb32"
      }
    }
  ]
}